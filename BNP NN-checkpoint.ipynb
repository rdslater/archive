{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from lasagne.layers import InputLayer, DropoutLayer, DenseLayer\n",
    "from lasagne.updates import nesterov_momentum\n",
    "from lasagne.objectives import binary_crossentropy\n",
    "from lasagne.init import Uniform\n",
    "from nolearn.lasagne import NeuralNet, BatchIterator\n",
    "import theano\n",
    "from theano import tensor as T\n",
    "from theano.tensor.nnet import sigmoid\n",
    "\n",
    "\n",
    "class AdjustVariable(object):\n",
    "    def __init__(self, name, start=0.03, stop=0.001):\n",
    "        self.name = name\n",
    "        self.start, self.stop = start, stop\n",
    "        self.ls = None\n",
    "\n",
    "    def __call__(self, nn, train_history):\n",
    "        if self.ls is None:\n",
    "            self.ls = np.linspace(self.start, self.stop, nn.max_epochs)\n",
    "\n",
    "        epoch = train_history[-1]['epoch']\n",
    "        new_value = np.float32(self.ls[epoch - 1])\n",
    "        getattr(nn, self.name).set_value(new_value)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn import cross_validation\n",
    "from sklearn import ensemble\n",
    "\n",
    "\n",
    "print('Load data...')\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "target = train['target'].values\n",
    "labels = train[\"target\"]\n",
    "train = train.drop(['ID','target','v8','v23','v25','v31','v36','v37','v46','v51','v53','v54','v63','v73','v75','v79','v81','v82','v89','v92','v95','v105','v107','v108','v109','v110','v116','v117','v118','v119','v123','v124','v128'],axis=1)\n",
    "#train = train.drop(['ID','target'],axis=1)\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "id_test = test['ID'].values\n",
    "trainId = target\n",
    "testId = test[\"ID\"]\n",
    "test = test.drop(['ID','v8','v23','v25','v31','v36','v37','v46','v51','v53','v54','v63','v73','v75','v79','v81','v82','v89','v92','v95','v105','v107','v108','v109','v110','v116','v117','v118','v119','v123','v124','v128'],axis=1)\n",
    "#test = test.drop(['ID'],axis=1)\n",
    "print('Clearing...')\n",
    "for (train_name, train_series), (test_name, test_series) in zip(train.iteritems(),test.iteritems()):\n",
    "    if train_series.dtype == 'O':\n",
    "        #for objects: factorize\n",
    "        train[train_name], tmp_indexer = pd.factorize(train[train_name])\n",
    "        test[test_name] = tmp_indexer.get_indexer(test[test_name])\n",
    "        #but now we have -1 values (NaN)\n",
    "    else:\n",
    "        #for int or float: fill NaN\n",
    "        tmp_len = len(train[train_series.isnull()])\n",
    "        if tmp_len>0:\n",
    "            #print \"mean\", train_series.mean()\n",
    "            train.loc[train_series.isnull(), train_name] = -999 \n",
    "        #and Test\n",
    "        tmp_len = len(test[test_series.isnull()])\n",
    "        if tmp_len>0:\n",
    "            test.loc[test_series.isnull(), test_name] = -999\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train = np.asarray(train, dtype=np.float32)        \n",
    "labels = np.asarray(labels, dtype=np.int32).reshape(-1,1)\n",
    "\n",
    "net = NeuralNet(\n",
    "    layers=[  \n",
    "        ('input', InputLayer),\n",
    "        ('dropout0', DropoutLayer),\n",
    "        ('hidden1', DenseLayer),\n",
    "        ('dropout1', DropoutLayer),\n",
    "        ('hidden2', DenseLayer),\n",
    "        ('output', DenseLayer),\n",
    "        ],\n",
    "\n",
    "    input_shape=(None, len(train[1])),\n",
    "    dropout0_p=0.1,\n",
    "    hidden1_num_units=50,\n",
    "    hidden1_W=Uniform(),\n",
    "    dropout1_p=0.2, \n",
    "    hidden2_num_units=40,\n",
    "    #hidden2_W=Uniform(),\n",
    "\n",
    "    output_nonlinearity=sigmoid,\n",
    "    output_num_units=1, \n",
    "    update=nesterov_momentum,\n",
    "    update_learning_rate=theano.shared(np.float32(0.01)),\n",
    "    update_momentum=theano.shared(np.float32(0.9)),    \n",
    "    # Decay the learning rate\n",
    "    on_epoch_finished=[AdjustVariable('update_learning_rate', start=0.01, stop=0.0001),\n",
    "                       AdjustVariable('update_momentum', start=0.9, stop=0.99),\n",
    "                       ],\n",
    "    regression=True,\n",
    "    y_tensor_type = T.imatrix,                   \n",
    "    objective_loss_function = binary_crossentropy,\n",
    "    #batch_iterator_train = BatchIterator(batch_size = 256),\n",
    "    max_epochs=20, \n",
    "    eval_size=0.1,\n",
    "    #train_split =0.0,\n",
    "    verbose=2,\n",
    "    )\n",
    "    \n",
    "print(net.batch_iterator_train)\n",
    "\n",
    "seednumber=1235\n",
    "np.random.seed(seednumber)\n",
    "net.fit(train, labels)\n",
    "\n",
    "\n",
    "preds = net.predict_proba(test)[:,0] \n",
    "\n",
    "\n",
    "submission = pd.read_csv('../input/sample_submission.csv')\n",
    "submission[\"PredictedProb\"] = preds\n",
    "submission.to_csv('NNbench.csv', index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
