{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "I worked in manufacturing as a process engineer for a long time.  One of the things I thought might be happening is sometimes a measurement is repeated on a part in different locations.  Given the the Ln_Sn seemed to be much more compact, I decided to try and compact the data (psuedo feature engineering) to get the data into a size more manageable.\n\nAlthough I cannot run this on kaggle as I save 2 intermediate files, the concept has worked well in a fairly quick manner--process time on my E3400/8GB was about 10 mins total from start to end.\n\nThere will be errors at the end, mainly as I save two intermediate files and then read from them later.  My apoologies--if I could suppress the errors I would.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \nimport pandas as pd\nimport numpy as np\nimport re\n# i keep my data in specific folders, modify for yourself\n# Lets read in the data by chunks --i report only train here, obviously one will \n#need to do this for test as well.\n\ntrain = pd.read_csv(\"../input/train_numeric.csv\", chunksize=100000)\ntrain_new=pd.DataFrame()\n#the first chunk I need to get column names and open the file, after that I will just append the file\ntoggle=0\n\n\nfor chunk in train:\n#next line just shows me this is working, and gives a rough row count\n    print(\"-\",end='')\n#for the first chunk, lets create some aggregate features.  Specifically, I am consolidating \n#the measurements for each \"Ln_Sn\" group where n=number.\n    if toggle == 0:\n        a=chunk.columns.values.tolist()\n        myString = '(L[0-9]*_S[0-9]*_) *'\n        pattern1 = r'\\b' + re.escape(myString) + r'\\b'\n        pattern=re.compile(myString)\n        indices = [i for i, x in enumerate(a) if re.search(pattern, x)]\n        i=0\n        for item in a:\n            if 'L' in item:\n                a[i]=re.search(pattern, a[i]).group(1)\n            i=i+1\n#this next line is a super sneaky way to remove all my duplicates\n        a=list(set(a))\n    new_train=pd.DataFrame()\n    for item in a:\n#ignopre the ID and response columns\n        if (item=='Id' )| (item=='Response'):\n            new_train[item]=chunk[item]\n#aggregate the columns with similar settings.\n#here I get the mean, max, min, and count of NAs.  Modify at your pleasure\n        else:\n            spike_cols = [col for col in chunk.columns if item in col]\n            mean=item+'mean'\n            nacount=item+'NAcount'\n            max_v=item+'max'\n            min_v=item+'min'\n            new_train[mean]=chunk[spike_cols].mean(axis=1)\n            new_train[nacount]=chunk[spike_cols].isnull().sum(axis=1)\n            new_train[max_v]=chunk[spike_cols].max(axis=1)\n            new_train[min_v]=chunk[spike_cols].min(axis=1)\n            cols=new_train.columns.values.tolist()\n            cols.sort()\n            new_train=new_train[cols]\n#now I will open or append a file with my condensed data. \n#remember to use your own particular path.\n    if toggle==0:\n        new_train.to_csv(\"train_num_smry.csv\",mode='w',index=False,header=True)\n        toggle=1\n    else:\n        new_train.to_csv( \"train_num_smry.csv\",mode='a',index=False,header=False)",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "-\n-"
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "Now I have two files train_num_smry.csv and the test couterpart--both very manageable in size.  Lets whip out our favorite classifier (I know all the cool kids use XGB, so I have to be different--but at this point, your data is a more manageable size).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "#use your favorite classifier here\nfrom sklearn.ensemble import ExtraTreesClassifier\n#read in your new file!\ntrain=pd.read_csv(\"train_num_smry.csv\")\n\nclf = ExtraTreesClassifier(n_estimators=10, max_depth=None, min_samples_split=1, \n                           random_state=0, criterion='entropy',verbose=1,n_jobs=-1)\n\ntarget=train['Response']\nids=train['Id']\ntrain=train.drop(['Response','Id'],axis=1)\ntrain=train.fillna(value=-100)\nclf.fit(train,target)\ntest=pd.read_csv(\"test_num_smry.csv\")\ntest=test.fillna(value=-100)\nids=test['Id']\ntest=test.drop(['Id'],axis=1)\noutput=clf.predict(test)\nsubmit=pd.DataFrame({\"Id\": ids, \"Response\":output})\nsubmit.to_csv(\"../output/xtra_tress_n10.csv\",index=False)\n\n##this got me on the LB ~ 0.19  Not great, but it was a start.",
   "execution_count": null,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "Obviously the dates and categorical are still missing, but this is one way to \"compress\" the data \nfor lesser hardware.",
   "metadata": {}
  }
 ]
}